{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d55490-3b9f-46db-b3e2-3ea4a7e3b2ae",
   "metadata": {},
   "source": [
    "# Módulo 24 exercício 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b364d5-5b0c-4661-ba37-646ddb30e68d",
   "metadata": {},
   "source": [
    "### 1 Diferenças entre AdaBoost e GBM\n",
    "\n",
    "1. **Foco nos Erros**: AdaBoost ajusta pesos das amostras com base nos erros das iterações anteriores, enquanto GBM otimiza uma função de perda, corrigindo os erros residuais diretamente.\n",
    "2. **Combinação de Modelos**: AdaBoost atribui pesos aos modelos base com base em sua precisão, enquanto GBM soma os outputs dos modelos para minimizar o erro global.\n",
    "3. **Velocidade**: AdaBoost é mais rápido devido à sua simplicidade, enquanto GBM é mais lento, pois envolve cálculos de gradiente.\n",
    "4. **Sensibilidade a Outliers**: AdaBoost é mais sensível a outliers, pois tenta corrigir cada erro, enquanto GBM é menos sensível devido ao uso de funções de perda robustas.\n",
    "5. **Flexibilidade**: GBM é mais flexível, permitindo personalização da função de perda, enquanto AdaBoost tem menos opções de personalização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1a75e-0f29-4ef8-8bbc-b4498459cc49",
   "metadata": {},
   "source": [
    "### 2- Cinco Hiperparâmetros Importantes no Gradient Boosting Machine (GBM)\n",
    "\n",
    "1. **n_estimators**: Número de árvores (ou modelos) a serem construídas. Um valor maior pode melhorar a performance, mas aumenta o tempo de treino e pode causar overfitting.\n",
    "2. **learning_rate**: Controla o impacto de cada árvore no modelo final. Valores menores tornam o aprendizado mais lento, mas podem melhorar a generalização.\n",
    "3. **max_depth**: Profundidade máxima das árvores. Limita a complexidade das árvores para evitar overfitting.\n",
    "4. **min_samples_split**: Número mínimo de amostras necessário para dividir um nó. Valores maiores ajudam a prevenir overfitting.\n",
    "5. **subsample**: Proporção de amostras usadas para treinar cada árvore. Usar menos de 1.0 pode introduzir aleatoriedade e ajudar na generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb2ae7-069b-4e01-bcb7-d8fe3f0dcfe2",
   "metadata": {},
   "source": [
    "### 3- exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9716bda-a743-401a-b3ca-77da26a83de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2 \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b61726-a575-492b-bbc4-395b90e48a6c",
   "metadata": {},
   "source": [
    "### 3.1 Explicação do Código\n",
    "\n",
    "Este código demonstra como utilizar o **Gradient Boosting Classifier** para classificação em dados gerados com `make_hastie_10_2`. \n",
    "\n",
    "1. **Gerando Dados**:\n",
    "   - `make_hastie_10_2` cria um conjunto de dados artificial com 10 variáveis independentes (X) e uma variável dependente binária (y).\n",
    "   - O parâmetro `random_state` garante que os dados gerados sejam reproduzíveis.\n",
    "\n",
    "2. **Divisão dos Dados**:\n",
    "   - O conjunto de dados é dividido em **treino** (primeiros 2000 exemplos) e **teste** (os 2000 exemplos restantes).\n",
    "\n",
    "3. **Definição do Classificador**:\n",
    "   - O `GradientBoostingClassifier` é configurado com:\n",
    "     - `n_estimators=100`: Número de árvores a serem treinadas.\n",
    "     - `learning_rate=1.0`: Taxa de aprendizado que controla o impacto de cada árvore no modelo final.\n",
    "     - `max_depth=1`: Profundidade máxima de cada árvore, limitando sua complexidade.\n",
    "     - `random_state=0`: Para garantir reprodutibilidade.\n",
    "\n",
    "4. **Treinamento e Avaliação**:\n",
    "   - O modelo é ajustado nos dados de treino usando `fit(X_train, y_train)`.\n",
    "   - O desempenho do modelo é avaliado no conjunto de teste com `score(X_test, y_test)`, retornando a pontuação de acurácia.\n",
    "\n",
    "Esse código exemplifica o uso básico do Gradient Boosting para classificação e como avaliar seu desempenho em um conjunto de dados de teste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25abd8e6-9a86-4937-b453-1e2e162dbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontuação no conjunto de teste: 0.913\n"
     ]
    }
   ],
   "source": [
    "# Gerando os dados com make_hastie_10_2\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definindo e treinando o classificador Gradient Boosting\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=1.0, \n",
    "    max_depth=1, \n",
    "    random_state=0\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o modelo\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Pontuação no conjunto de teste:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdceb3-13dd-48c2-ac75-99a4ec146cea",
   "metadata": {},
   "source": [
    "### 3.2 Explicação do Resultado\n",
    "\n",
    "A **pontuação no conjunto de teste** é **0.913**, o que significa que o modelo obteve uma acurácia de **91,3%** ao classificar corretamente os dados de teste. Isso indica que o modelo foi capaz de generalizar bem para dados não vistos, sugerindo um bom ajuste durante o treinamento e validação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88569094-9978-4b8d-915f-cdbd0e8c0f1d",
   "metadata": {},
   "source": [
    "### 4- Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb519a-54d9-44af-b0ee-0b97bc5481ca",
   "metadata": {},
   "source": [
    "### Explicação do Código\n",
    "\n",
    "Este código utiliza o **GridSearchCV** para encontrar os melhores hiperparâmetros do modelo **Gradient Boosting Classifier** no conjunto de dados gerado pelo `make_hastie_10_2`.\n",
    "\n",
    "1. **Gerando Dados**:\n",
    "   - `make_hastie_10_2` gera um conjunto de dados artificial para classificação binária.\n",
    "   - O conjunto é dividido em **treino** (2000 amostras) e **teste** (2000 amostras restantes).\n",
    "\n",
    "2. **Definição do Modelo**:\n",
    "   - Um modelo básico de `GradientBoostingClassifier` é criado sem especificar os hiperparâmetros, pois estes serão ajustados pelo **GridSearchCV**.\n",
    "\n",
    "3. **Grade de Hiperparâmetros**:\n",
    "   - Testam-se diferentes combinações de hiperparâmetros, como:\n",
    "     - `n_estimators`: Número de árvores no modelo.\n",
    "     - `learning_rate`: Taxa de aprendizado que controla o impacto de cada árvore.\n",
    "     - `max_depth`: Profundidade máxima de cada árvore.\n",
    "     - `min_samples_split`: Mínimo de amostras necessárias para dividir um nó.\n",
    "     - `subsample`: Proporção de amostras usadas para cada árvore.\n",
    "\n",
    "4. **Validação Cruzada com GridSearchCV**:\n",
    "   - O **GridSearchCV** realiza validação cruzada (5 folds) para testar todas as combinações de hiperparâmetros definidos na grade.\n",
    "   - O melhor conjunto de hiperparâmetros é armazenado em `grid_search.best_params_`.\n",
    "\n",
    "5. **Avaliação do Modelo**:\n",
    "   - O modelo com os melhores hiperparâmetros (`grid_search.best_estimator_`) é avaliado no conjunto de teste.\n",
    "   - A pontuação (acurácia) do modelo no conjunto de teste é exibida.\n",
    "\n",
    "Este processo otimiza o desempenho do modelo ajustando seus hiperparâmetros, garantindo uma melhor generalização nos dados de teste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af71c85-fc7a-41fd-b01b-6f5923904f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'learning_rate': 1.0, 'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Pontuação no conjunto de teste com os melhores hiperparâmetros: 0.9279\n"
     ]
    }
   ],
   "source": [
    "# Gerando os dados com make_hastie_10_2\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definindo o classificador Gradient Boosting\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Configurando a grade de hiperparâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Número de árvores\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],  # Taxa de aprendizado\n",
    "    'max_depth': [1, 3, 5],  # Profundidade máxima das árvores\n",
    "    'min_samples_split': [2, 5, 10],  # Mínimo de amostras para dividir um nó\n",
    "    'subsample': [0.6, 0.8, 1.0]  # Proporção de amostras para cada árvore\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajustando o modelo para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "\n",
    "# Avaliando o modelo com os melhores hiperparâmetros no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "score = best_model.score(X_test, y_test)\n",
    "print(\"Pontuação no conjunto de teste com os melhores hiperparâmetros:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170eb39-296a-400f-b965-ae764bceb736",
   "metadata": {},
   "source": [
    "### Explicação dos Resultados\n",
    "\n",
    "- **Melhores Hiperparâmetros**:\n",
    "  - `learning_rate`: 1.0 — Cada árvore contribui com sua previsão total, maximizando o impacto individual.\n",
    "  - `max_depth`: 1 — Árvores rasas (stumps), reduzindo o risco de overfitting e garantindo simplicidade.\n",
    "  - `min_samples_split`: 2 — Nós podem ser divididos com o menor número possível de amostras.\n",
    "  - `n_estimators`: 150 — Um maior número de árvores, fornecendo melhor desempenho sem overfitting.\n",
    "  - `subsample`: 1.0 — Utiliza todo o conjunto de dados em cada iteração, mantendo estabilidade.\n",
    "\n",
    "- **Pontuação no Conjunto de Teste**:\n",
    "  - A acurácia obtida foi **92,79%**, indicando que o modelo otimizado com esses hiperparâmetros generalizou bem e teve um desempenho ligeiramente melhor em comparação ao modelo inicial, que obteve 91,3%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4337415-97cf-4bf6-b75f-ea12071aabc9",
   "metadata": {},
   "source": [
    "### Diferença entre Gradient Boosting e Stochastic Gradient Boosting\n",
    "\n",
    "A principal diferença entre o **Gradient Boosting** e o **Stochastic Gradient Boosting** está na introdução da **aleatoriedade** no processo de treinamento do Stochastic Gradient Boosting:\n",
    "\n",
    "- **Gradient Boosting**: Utiliza todo o conjunto de dados de treino para ajustar cada árvore de decisão. É um processo determinístico, onde o modelo minimiza o erro gradualmente em cada iteração, sem incorporar variabilidade.\n",
    "\n",
    "- **Stochastic Gradient Boosting**: Introduz uma camada de aleatoriedade ao selecionar uma **subamostra aleatória** do conjunto de dados em cada iteração (sem reposição). Isso reduz a correlação entre as árvores, melhora a generalização do modelo e aumenta a robustez contra overfitting.\n",
    "\n",
    "Essa aleatoriedade torna o Stochastic Gradient Boosting mais eficiente em termos de tempo computacional e mais robusto ao lidar com conjuntos de dados grandes ou ruidosos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
